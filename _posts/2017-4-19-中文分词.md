---
layout: post
title: 中文分词
description: 深度学习-中文分词
category: 深度学习-中文分词
---

## 97.5%准确率的深度学习中文分词（字嵌入+Bi-LSTM+CRF） 

## 摘要

深度学习当前在NLP领域发展也相当快，翻译，问答，摘要等基本都被深度学习占领了。 本文给出基于深度学习的中文分词实现，借助大规模语料，不需要构造额外手工特征，在2014年人民日报语料上取得97.5%的准确率。模型基本是参考论文：http://www.aclweb.org/anthology/N16-1030
## 相关方法

中文分词是个比较经典的问题，各大互联网公司都会有自己的分词实现。 考虑到性能，可维护性，词库更新，多粒度，以及其他的业务需求，一般工业界中文分词方案都是基于规则。

1） 基于规则的常见的就是最大正/反向匹配，以及双向匹配。
2） 规则里糅合一定的统计规则，会采用动态规划计算最大的概率路径的分词
以上说起来很简单，其中还有很多细节，比如词法规则的高效匹配编译，词库的索引结构等。
3） 基于传统机器学习的方法 ，以CRF为主，也有用svm，nn的实现，这类都是基于模型的，跟本文一样，都有个缺陷，不方便增加用户词典（但可以结合，比如解码的时候force-decode）。 速度上会有损耗。 另外都需要提取特征。传统CRF一般是定义特征模板，方便性上有所提高。另外传统CRF训练算法(LBFGS)较慢，也有使用sgd的，但多线程都支持的不好。代表有crf++, crfsuite, crfsgd, wapiti等。
## 深度学习方法

深度学习主要是特征学习，端到端训练， 适合有大量语料的场景。另外各种工具越来越完善，利用GPU可大幅提高训练速度。

前文提过，深度学习主要是特征学习，在NLP里各种词嵌入是一种有效的特征学习。 本文实现的第一步也是对语料进行处理，使用word2vec对语料的字进行嵌入，每个字特征为50维。

得到字嵌入后，用字嵌入特征喂给双向LSTM， 对输出的隐层加一个线性层，然后加一个CRF就得到本文实现的模型。

![1](http://jackwangmeng.github.io/picture/2017-4-19_.png)


另外，字符嵌入的表示可以是纯预训练的，但也可以在训练模型的时候再fine-tune,一般而言后者效果更好。


对于fine-tune的情形，可以在字符嵌入后，输入双向LSTM之前加入dropout进一步提升模型效果。


最后，对于最优化方法，文本语言模型类的貌似Adam效果更好， 对于分类之类的，貌似AdaDelta效果更好。
## 语料

本文使用2014人民日报语料，一共50w+ 句子,1千多万的字符次数 (句子长度超过50的不考虑)

标注示例：

    法新社/j 报道/v 说/v ，/w [泰国/nsf 政府/nis]/nt 已经/d 作/v 好/a 签发/v 紧急状态/n 令/v的/ude1 准备/vn 。/w （/w 老/a 任/v ）/w

## 预处理

我们首先使用word2vec对字进行嵌入，具体就是把每一句按字符切割，空格隔开，喂给word2vec,指定维度50

然后我们把每一句处理成 :

    字索引1 字索引2 … 字索引N 标注1 标注2 … 标注N

对于标注，我们按字分词的典型套路，

    对于单独字符，不跟前后构成词的，我们标注为S (0)

    跟后面字符构成词且自身是第一个字符的，我们标注为B (1)

    在成词的中间的字符，标注为M (2)

    在词尾的字符，标注为E (3)

这样处理后使用前面描述模型训练。
## 训练代码
![2](http://jackwangmeng.github.io/picture/2017-4-19_2.png)
![2](http://jackwangmeng.github.io/picture/2017-4-19_3.png)
![2](http://jackwangmeng.github.io/picture/2017-4-19_4.png)
![2](http://jackwangmeng.github.io/picture/2017-4-19_5.png)
![2](http://jackwangmeng.github.io/picture/2017-4-19_6.png)
![2](http://jackwangmeng.github.io/picture/2017-4-19_7.png)

## 展望开源

现在在尝试用深度学习来做各种NLP的实验，但是苦于现在已有的带GPU的机器的性能和大量任务。如果有哪位欣赏现在工作的金主，能捐赠一台价值两万（或是各位施主的打赏，哈哈）的GTX 1080机器，本人就开源现在的分词项目，和以后的其它NLP实验项目。
